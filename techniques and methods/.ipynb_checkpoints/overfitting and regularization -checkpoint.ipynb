{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d17ee18",
   "metadata": {},
   "source": [
    "## Overfitting \n",
    "if we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples. \n",
    "\n",
    "## Regularization \n",
    "For housing price regression problem , we may have 100 featues and there for 101 parameters including bias. But we dont know  which weights are relevant.In regularization we take the cost function and add regularization terms at the end to shrink all the parameters. by convention we dont penalize the bias term. ( ie ,100 parameters) \n",
    "This will make the hypothesis realatively simple.\n",
    "<img src = 'https://www.holehouse.org/mlclass/07_Regularization_files/Image%20[5].png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d73ee",
   "metadata": {},
   "source": [
    "If we set the lambda too high then that will lead to underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8040b",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abf845",
   "metadata": {},
   "source": [
    "Non invertability issue of closed form solution in linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7693f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
