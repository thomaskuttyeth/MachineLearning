{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2dacc74",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ac1de",
   "metadata": {},
   "source": [
    "$y=mx+b$\n",
    "\n",
    "$b= \\frac{1}{n}\\left(\\sum y_i-m\\sum x_i\\right)$\n",
    "\n",
    "$m = \\frac{S_{xy}}{S_{xx}}= \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}$\n",
    "\n",
    "================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe8c7e",
   "metadata": {},
   "source": [
    "$y_i = \\boldsymbol{w}^T\\boldsymbol{x}_i + \\varepsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba959c",
   "metadata": {},
   "source": [
    "$\\hat{\\boldsymbol{w}}=\\left[\\boldsymbol{X}^T\\boldsymbol{X} \\right]^{-1}\\boldsymbol{X}^T\\boldsymbol{y},$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4031a",
   "metadata": {},
   "source": [
    "## Questions\n",
    "0. fundamental idea of linear regression \n",
    "1. Basic formulation of regression equation ( intercept, slope, error, actual and estimated equations)\n",
    "2. How we estimate weights\n",
    "3. least sqaure method --- partial differntial ---> minimising the sqaured error\n",
    "4. y predicted \n",
    "5. residual--- > \n",
    "6. RSS = TSS - ESS --> \n",
    "7. r2 tends to 1 (condition) \n",
    "8. why we need adjusted r2\n",
    "9. hypothesis testing for checking betas = 0\n",
    "10. model overall testing - Anova - ( f value) \n",
    "11. significant features avoiding - what will happen -- (model specification errors)  \n",
    "12. non significant featues including --- errror ( model specification erros)\n",
    "13. gradient descent optimization for fining best weights \n",
    "14. gradient descent updation formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238ea41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## OLS ASSUMPTIONS \n",
    "\n",
    "\n",
    "1. No multicollinearity \n",
    "2. No auto correlation \n",
    "3. No heteroskedasticity. \n",
    "4. Normality \n",
    "5. Exogenity: independent variables are not correlated with the error term. \n",
    "              it arises due to the omission of explanatory variables in the regression.  \n",
    "\n",
    "\n",
    "6. linear in paramters \n",
    "\n",
    "Note : (error terms should be \n",
    "        idependently (no autocorrelation)  and \n",
    "        identically(normality) distributed with \n",
    "        constant variance ( homoskedastic) \n",
    "        and zero mean) \n",
    "\n",
    "\n",
    "## ols violations \n",
    "\n",
    "1. multicollinearity \n",
    "========================================================================\n",
    "two or more independent explanatory variables have high correlation among them. \n",
    "if we have this problem, the we cannot estimate the effect of each predictor variable on the dependent variable.\n",
    "multicollinearity increases the variance of the coefficient, this further increases the width of confidence interval. \n",
    "\n",
    "we can combine both variables, or use alternate specifications. \n",
    "\n",
    "* conventional diagoniss is by using vif ( variance inflation factor)\n",
    "it assesses how much variance of an estimated regression coefficient increases if predictors in a model are correlated. \n",
    "\n",
    "vif(beta1)  = 1/ (1-r2) , tolerance = 1/vif ,  where r2 refers to the multiple correlation coefficient vetween x1 and \n",
    "other predictor variables.\n",
    "if vif > 5 then it refers to the presence of multicollinearity. \n",
    "\n",
    "warnings signs of multicollinearity: \n",
    "    * high r2 but insignificant \n",
    "    * coefficient are opposite signs of their expected. \n",
    "    * add or remove one fetaure the regression coefcient changes dramatically. \n",
    "    * add or delete observation the regression coefficients may change substantially. \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "2. Heteroskedasticity : violation of constant variance of error term. \n",
    "=====================================================================\n",
    "* heteroskedasticity refers to situvations where the variance of the residuals is unequal over \n",
    "a range of measured values. when running a regression analysis, heteroskedasticity results in an unqual scatter \n",
    "of the residuals. This is the violation of the assumption of constant variance. \n",
    "\n",
    "* we can analyse this from the graph of residuals. \n",
    "* formally we diagnose this using breusch-pagan test. \n",
    "    h0: the residuals are homoskedastic \n",
    "    ha: the residuals are heteroskedastic. \n",
    "* remedial measures\n",
    "    *alternate methods of estimation \n",
    "    * deflating data ( log transformation, ) \n",
    "    * robust errors \n",
    "\n",
    "\n",
    "\n",
    "3. Non-normality: violationo of identical distribution of error terms\n",
    "==================================================================================\n",
    "* calculation of confidence intervals and various significance tests for coefficients are all\n",
    " based on the assumptions of normally distributed errors. least sqaure estimators are \n",
    " unbiased and mvue estimator. These properties hold even if the errors do not have the \n",
    " normally distribution provided other assumptions are satisfied. \n",
    " \n",
    "* in order to get the confidence intervals for alpha and beta and test any hypothesis about\n",
    " alpha and beta we need the normality of errors assumption.\n",
    "  \n",
    "* detection is by creating QQ Plot. ========\n",
    " it is a scatter plot created by plotting two sets of quantiles against one another.\n",
    " one drawn form a theoretical normal distribution and the other drawn form the residuals of the model.  \n",
    " if both sets of quantiles come from the same distribution, the points will form a roughly straight line. \n",
    " Qantiles : points in your data below which a certain proportion of your data falls. \n",
    "\n",
    "\t*identify the source of non-normality :\n",
    "\t\toutliers and non-normal distribution\n",
    "\t\t\t* respecification of model( alternate functional)  \n",
    "\t\t\t* logs \n",
    "\t\t\t* omitted explanotory variables \n",
    "\t\t\t\n",
    "\t\t\t* box-cox transformation: transformation of non-normal dependent variable into normal\n",
    "\t\t\t shape. at the core of the box cox transformation is the exponent lambda which varies\n",
    "\t\t\t  from -5 to 5. all the values of lambda are considered and optimal vlaue for your \n",
    "\t\t\t  data is selecteed. the optimal vlaue is the one which results in the best\n",
    "\t\t\t   approximatio of a normal distribution curve.if the lambda is 1 then no \n",
    "\t\t\t   transformation is required. if lambda is zero then log(y). \n",
    "\t\t\t\n",
    "\n",
    "4. serieal correlation or auto correlation: violation of independently distributed error terms \n",
    "====================================================================================================\n",
    "\trelationship between a given variable and a lagged version of itself over various\n",
    "\t time intervals. ( usually in time seies data) \n",
    "\tpresence of seriel correlation means that the errors are not independenly distrubuted.\n",
    "\t thus the error term at t is correlated to error term at some other perios say t-1 , t-2, ... t-k. \n",
    "\tseial correlation is due to the correlation of omitted variable that the error term captures.\n",
    "\t serial correlation leads to inefficient ols estimators and we can not rely on test of significance. \n",
    "\t\n",
    "\t* auto correlation diagonosis is through durbin watson test. \n",
    "\tdw = 2(1-corr) \n",
    "\th0: no first order auto correlation \n",
    "\th1: there exist first order auto correlation \n",
    "\t\n",
    "\tdurbin watson statistic ranges from 0 to 4 , where \n",
    "\t\t* 2 is no autocorrelation \n",
    "\t\t* 0-2 positive auto correlation \n",
    "\t\t* 2-4 negative auto correlation \n",
    "\t\t* note: 1.5 to 2.5  is relatively normal. \t\t\n",
    "\t\tTo test for positive autocorrelation at significance level α (alpha), the test statistic \n",
    "\t\tDW is compared to lower and upper critical values:\n",
    "\n",
    "\t\t* If DW < Lower critical value: There is statistical evidence that the data is positively autocorrelated\n",
    "\t\t* If DW > Upper critical value: There is no statistical evidence that the data is positively correlated.\n",
    "\t\t* If DW is in between the lower and upper critical values: The test is inconclusive.\t\n",
    "\t\t\t\n",
    "\t\t* other tests are durbin h test, durbin alternate test, lagrange multiplier test, \t\n",
    "\t\t\n",
    "\t\t* remedial measures ( using robust standered errors) \n",
    "\t\t\n",
    "\t\t\n",
    "### leverage / standardizes residuals / cooks distance \n",
    "\n",
    "\n",
    "Measures how far away data points are from the other observations. it determines the stregnth of\n",
    " sample value on the prediction. \n",
    "\n",
    "standardized residuals: meaure of the strength of the difference between observed and expected values. \n",
    "if an outlier is significant it will produce substantial changes in the regression equation estimates. \n",
    "\n",
    "Leverage is the distance from the mass  center of the data \n",
    "cook's distance is an overall measure of influence of an observation. \n",
    "Points with high leverage may be influential: that is, deleting them would change the model a lot. \n",
    "For this we can look at Cook’s distance, which measures the effect of deleting a point on the combined parameter vector\n",
    ". Cook’s distance is the dotted red line here, and points outside the dotted line have high influence. \n",
    "In this case there are no points outside the dotted line. \n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "# ANALYSIS OF RESIDUALS PLOTS\n",
    "\n",
    "### Analysis of Residuals vs Fitted \n",
    "\n",
    "  This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor\n",
    "  variables and an outcome variable and the pattern could show up in this plot if the model doesn't capture the \n",
    "  non-linear relationship. If you find equally spread residuals around a horizontal line without a specific pattern,\n",
    "  that is a good indication that  you don't have non-linear relationships. \n",
    "\t\n",
    "### Analysis of Normal Q-Q plot \n",
    "  This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate \n",
    "  severely?. It's good if residuals  are lined well on the straight dashed line. if the qq plot looks curved then the \n",
    "  residuals are skewed in one direction.  if you have heavy tails then also we have a non -normal distribution \n",
    "  flatter tails. we need normality assumption for model hypothesis tests all of this test assume that residuals \n",
    "  are normally distributed. \t\t\n",
    "\t\t\n",
    "###  Analysis of scale-location plot \n",
    "  It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors.\n",
    "   This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal \n",
    "   line with emqually (randomly) spread points.\n",
    "   \n",
    "###  Residuals vs leverage\n",
    "  Unlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner \n",
    "  or at the lower right corner. Those spots are the places where cases can be influential against a regression line. \n",
    "  Look for cases outside of a dashed line, Cook’s distance. When cases are outside of the Cook’s distance\n",
    "  (meaning they have high Cook’s distance scores), the cases are influential to the regression results. \n",
    "  The regression results will be altered if we exclude those cases.\t\t\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca0142",
   "metadata": {},
   "source": [
    "# REGULARIZATION METHODS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338b5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4215fc14",
   "metadata": {},
   "source": [
    "# EVALUATION METRICES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b113b1",
   "metadata": {},
   "source": [
    "--> https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a4fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
