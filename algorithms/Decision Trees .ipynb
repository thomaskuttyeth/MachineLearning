{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c936c5",
   "metadata": {},
   "source": [
    "# Decision Trees \n",
    " * It is a binary tree that recursively splits the datset until we are left with pure leaf nodes, that is the data with only one type of class. \n",
    " *  Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    " * It is true that the tree is just a bunch of if else statement, But there exist many such trees to split the given data, so we have to find the optimal one.Thats why we this is considered as a machine learning algorithm.\n",
    " * Model will choose the split which maximises the information gain. \n",
    " * Entropy is the measure of information contained in a state.If entropy is high, then we are very uncertain about the randomly picked point and we need more bits to descrive the statse.\n",
    " * Pure node is the node where entropy is minimum \n",
    " \n",
    " To decide which attribute should be at the root node we can start by splitting the dataset wrt each feature and compute the gini impurity score. Consider the following splits based on three features.  \n",
    " <img src='https://miro.medium.com/max/980/1*S4byQQs8X2rNhJv4qm73uw.png'>\n",
    " <img src = 'https://miro.medium.com/max/980/1*j7WkOMGSLd17P_wlO_cRhw.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a9f6",
   "metadata": {},
   "source": [
    "### Gini Impurity \n",
    "The Gini impurity is a measurement of the likelihood of incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set. The lower the Gini score the better.\n",
    "<img src = 'https://miro.medium.com/max/445/1*y78T1YV8wrb-DPbmNk2rKA.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28b892",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/980/1*3SMk52FU2a3TrDysEu7dkQ.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e69c6",
   "metadata": {},
   "source": [
    "Suppose “Fever” has the lowest Gini impurity with a score of 0.386. In other words, “Fever” separates patients with and without flu the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596b556",
   "metadata": {},
   "source": [
    "<img src = 'https://miro.medium.com/max/850/1*uvngcDnw_vla-jWnFyyIJg.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c2515",
   "metadata": {},
   "source": [
    "We can apply this process to the right node as well. If the node itself has the lowest score then there is no point in separating the patients anymore and the node becomes the leaf node. This method is applied down the tree until we are only left with leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8545add",
   "metadata": {},
   "source": [
    "## Decision Tree with Numeric Values\n",
    "What if the “Yes”/”No” answers in “Fever” were replaced with actual temperature readings ? How would we compile the tree then? Don’t panic, the process very similar to before. First we sort the values of the attribute in ascending order (smallest to largest). Then we compute the average for all adjacent patients. For example, if the first two values were 10 and 20, we’d get the average of those two values (average = (10+20)/2 = 15) and insert the value in between 10 and 20.\n",
    "<img src = 'https://miro.medium.com/max/904/1*EWeBpqTq7_XhqvnSJwsMjg.png'>\n",
    "Once we have inserted all the averages, we can use the averages to create a tree with a root node and two leaf nodes. In the picture above we have selected the value 37.5 (average of 37 and 38) and used it as the attribute in the root node. It asks the question: which patients have a fever less than 37.5 degrees celsius? If the answer is Yes (True) then it goes into the left node, otherwise it goes into the right node. Can you see that we follow the same notion as before. After fulfilling our node we must calculate the Gini impurity score for the node (with “Fever < 37.5” as the attribute). To obtain the Gini score we do the same as before: calculate Gini scores for the leaf nodes and then using weighted average methods we get the Gini impurtiy score for the root node.\n",
    "This process is done for all averages. The average which returns the lowest Gini impurity score is selected to be the cut-off value in the root node or parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f2e3d",
   "metadata": {},
   "source": [
    "## Notes: \n",
    " * decision tree contains two nodes decision nodes and leaf nodes. Decision nodes contain condition and the condition is defined by feature index and the threshold value for that particular feature. \n",
    " * The model tranverses through every feature and feature value for finding the best splitting condition which maximises the  information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74568455",
   "metadata": {},
   "source": [
    "## Entropy \n",
    "Entropy is a measure of information that indicates the disorder of the features with the target\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR9d1OavwqcVz7tyymnWsnFHXRbOoKZunUumy4O3ziO_n7PReea87ixH400xaSC5RWzXQ&usqp=CAU\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a44f8",
   "metadata": {},
   "source": [
    "## Information gain\n",
    "The difference between entropy before and after splitting\n",
    "<img src='https://qph.fs.quoracdn.net/main-qimg-dfad11c548327127fadd25ff992ace92'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e960b5",
   "metadata": {},
   "source": [
    "# Implimentation of decision tree from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6b4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb84feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        ''' function to compute entropy '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count) \n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde76a3",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20195f",
   "metadata": {},
   "source": [
    "The algorithm follows almost same process the difference lies in how we measure the information gain and how we make the prediction. Suppose our new data point reaches a particular leaf node. To make the prediction for regression problem we take the average value of y values in the node. \n",
    "\n",
    "To find the best split and threshold in decision tree regressor we use variance as a measure of impurity.(Just like entropy or gini index in the classification problem)\n",
    "\n",
    "$VarReduction  = Var(parent) - \\sum{w_i}* Var(child)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacc069",
   "metadata": {},
   "source": [
    "#### Note : Decision Tree Split for numerical variables \n",
    " * First we sort the feature then we go through all the unique feature values and compute the information gain.\n",
    " * decision tree takes a lot of time to train if we have numerical feature. \n",
    " * time complexity will be increasing as the input is increasing. \n",
    " * decision trees are the base of many ensemble tenchiques \n",
    " * Decision Trees are robust to outliers because decision trees divide items by lines, so it does not difference how far is a point from lines.Most likely outliers will have a negligible effect because the nodes are determined based on the sample proportions in each split region (and not on their absolute values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a862f6",
   "metadata": {},
   "source": [
    "## Practise Questions \n",
    "\n",
    " * How does decision tree works(entropy, inf gain, how to construct the tree, gini impurity) \n",
    " * Understand the formula \n",
    " * how to construct the tree with respect to categorical and numberical features \n",
    " * What is the impact of outliers in decision tree \n",
    " * Decision tree has low bias and high variance \n",
    " * what is low bias and high variance. \n",
    " * generalization error \n",
    " * bias variance trade off \n",
    " * full depth decision tree --> getting high variance \n",
    " * how to convert high variance to low variance \n",
    " * decision tree pruning \n",
    " * How to perform pruning \n",
    " * which all libraries you will use \n",
    " * visualizaiton stuffs \n",
    " * How decision tree regressor works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bbae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
