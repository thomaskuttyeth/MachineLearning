{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02485a4",
   "metadata": {},
   "source": [
    "## Radom Forest Algorithm \n",
    "* A random forest is a machine learning technique that’s used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many models. \n",
    "* A random forest algorithm consists of many decision trees.\n",
    "* The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or majority of the output from various trees. Increasing the number of trees increases the precision of the outcome\n",
    "* The main difference between the decision tree algorithm and the random forest algorithm is that establishing root nodes and segregating nodes is done randomly in the latter. The random forest employs the bagging method to generate the required prediction.\n",
    "* Bagging involves using different samples of data (training data) rather than just one sample. A training dataset comprises observations and features that are used for making predictions. The decision trees produce different outputs, depending on the training data fed to the random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b22c0",
   "metadata": {},
   "source": [
    "<img src = 'https://files.ai-pool.com/a/3406775c0c6f8fd9f8701c7ca671dad9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70724a",
   "metadata": {},
   "source": [
    "###  Bootstrapping \n",
    "Random sampling of training observations\n",
    "When training, each tree in a random forest learns from a random sample of the data points. The samples are drawn with replacement, known as bootstrapping, which means that some samples will be used multiple times in a single tree.\n",
    "\n",
    "##### Feature selection is also random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd4764",
   "metadata": {},
   "source": [
    "### Majority voting - aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b467cd1",
   "metadata": {},
   "source": [
    "* Why it is called random ? \n",
    "   - bootstrapping ( ensures that we are not useing same data for every data --> less sensitive to the original data) \n",
    "   - random feature selection (helps to reduce correlation between trees) --> reduces the variance \n",
    "   - there are some tress which gives bad prediction \n",
    "   \n",
    "* whats the ideal number of features? sqaure rooot of the total number of features or log of totl number of features\n",
    "* how we can use random forest for regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a511f3",
   "metadata": {},
   "source": [
    "## Bagging and Voting\n",
    " * Bagging stands for Bootstrap aggregating, is a way to decrease the variance of your predictions by generating additional data fro training from your original dateset using combinations with repetitions to produce multi-sets of the same cardinality/size as your original data.\n",
    "\n",
    " * Parallel ensemble : \n",
    "   * Each model is built independently Aim is to decrease the variance, not the bias\n",
    "   * Suitable for high variance and low bias models (complex models)\n",
    " * process    \n",
    "    * Initially we have a data set for training. And since this is ensemble learning we have multiple models here. lets say (M1, M2, M3,M4, … Mn) . For each model we provide a different samples of data and make predictions. Here we are using row sampling with replacement. So, for each base learners we have different samples.\n",
    "    * Finally when we given the test data to the model each base learners will predict some output. Then we apply voting classifier which means majority of the votes given by the classifier will be taken as the final prediction. Why this method is known as bootstrap aggregation ? The step we use row sampling with replacement is known as bootstrapping and the step where we applying the voting classifier is known as the aggregation. An example of a tree based method is random forest, which develop fully grown tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08e1de",
   "metadata": {},
   "source": [
    "## Questions \n",
    "1. differnct kinds of parameters in random forest \n",
    "2. what is ensemble learnining \n",
    "3. random forest regressor and classification \n",
    "4. examples of ensemble techniques \n",
    "5. How random forest works? \n",
    "6. Criterion of decision tree and random forest : gini, mse, entropy\n",
    "7. How the final output is getting \n",
    "8. what are the hyperparameters in random forest \n",
    "9.  How to visualize decision tree \n",
    "10. row sampling and column sampling \n",
    "11. start by explaining decision tree \n",
    "12. should be able to explain bias variance for ml model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b78bb",
   "metadata": {},
   "source": [
    "## Advantages of Random Forest Classifier \n",
    "1. doesnt overfit \n",
    "2. less parameter tuning \n",
    "3. decision tree can handle both continous and categorical variables \n",
    "4. no feature scaling ( bcz it uses decision tree) \n",
    "5. suitable for any kind of ml problems \n",
    "6. robust to outliers \n",
    "\n",
    "#### Disadvantages \n",
    "1. all the disadvantages of decision trees\n",
    "2. biased with features having many categories \n",
    "3. biased in multiclass classification problems towards more frequent classes \n",
    "\n",
    "#### Type of problems it can solve ( supervised problems )\n",
    "1. classification \n",
    "2 .regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851b4ae",
   "metadata": {},
   "source": [
    "### Practical Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46167edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
